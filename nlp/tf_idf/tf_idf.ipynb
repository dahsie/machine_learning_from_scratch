{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as linalg\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unidecode\n",
    "from typing import List\n",
    "from word2number import w2n\n",
    "import re\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['data science is one of the most important fields of science',\n",
    "          'this is one of the best data science courses',\n",
    "          'data scientists analyze data' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data science is one of the most important fields of science'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = corpus[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525600\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi SiÃ©, give me five hundred twenty five thousand six hundred\"\n",
    "print(w2n.word_to_num(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data science is one of the most important fields of science'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'important',\n",
       " 'fields',\n",
       " 'of',\n",
       " 'science']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'important',\n",
       " 'fields',\n",
       " 'of',\n",
       " 'science']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.lower().split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gfg is best for  Geeks \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "test_str = 'Gfg, is best: for ! Geeks ;'\n",
    "\n",
    "test_str = test_str.translate(str.maketrans('', '',\n",
    "                                    string.punctuation))\n",
    "print(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'getting into'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "# lem.lemmatize(\"getting into\", pos=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizerPipeline:\n",
    "\n",
    "    def __init__(self, \n",
    "                lemmatize = True, \n",
    "                stop_words_use = True, \n",
    "                language = 'english',\n",
    "                punctuation_removal = True,\n",
    "                lowercase = True,\n",
    "                unidecode_use = True,\n",
    "                tokenize = True):\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer() if lemmatize else None\n",
    "        self.punctuation_table = str.maketrans('','',string.punctuation) if punctuation_removal else None\n",
    "        self.tokenizer = word_tokenize if tokenize else None\n",
    "        self.stop_words = set(stopwords.words(language)) if stop_words_use else None\n",
    "        self.unicode = unidecode.unidecode  if unidecode_use else None\n",
    "\n",
    "        self.stop_words_use = stop_words_use\n",
    "        self.lowercase = lowercase\n",
    "        self.punctuation_removal = punctuation_removal\n",
    "        self.tokenize = tokenize\n",
    "        self.lemmatize = lemmatize\n",
    "        self.unicode_use = unidecode\n",
    "    \n",
    "    def normalize_text(self,doc : str):\n",
    "        \n",
    "        text = re.sub(' +',' ', doc) # deleting extra white space\n",
    "\n",
    "        if self.unicode_use :\n",
    "            text = self.unicode(text)\n",
    "        if self.lowercase :\n",
    "            text = text.lower()\n",
    "        if self.punctuation_removal:\n",
    "            text = text.translate(self.punctuation_table)\n",
    "\n",
    "        text = text.strip()\n",
    "\n",
    "        words = self.tokenizer(text) if self.tokenize else text.split()\n",
    "        \n",
    "        if self.stop_words_use :\n",
    "            words = [word for word in words if word not in self.stop_words]\n",
    "        \n",
    "        if self.lemmatize :\n",
    "            words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "      \n",
    "        return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfidfVectorizer:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.features: List[str] = []\n",
    "\n",
    "    def tf(self,term: str, doc: str) -> float:\n",
    "    \n",
    "        tokens = word_tokenize(doc)\n",
    "        return tokens.count(term)/len(tokens)\n",
    "\n",
    "    def idf(self,term: str, docs: List[str]) -> float :\n",
    "        \n",
    "        frequence = 0\n",
    "        frequence = sum([1 for doc in docs if term in word_tokenize(doc)])\n",
    "\n",
    "        return np.log10((1+len(docs)) / (1+ frequence)) +1 \n",
    "\n",
    "    def vocabulary(self,docs : List[str]) -> List[str]:\n",
    "        set_words = set()\n",
    "        for doc in docs:\n",
    "            set_words = set_words.union(set(word_tokenize(doc)))\n",
    "        return list(set_words)\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.features\n",
    "    \n",
    "    def tf_idf(self,docs: List[str]) -> pd.DataFrame:\n",
    "        \n",
    "        self.features = self.vocabulary(docs=docs)\n",
    "        lines= len(docs)\n",
    "        cols = len(self.features)\n",
    "\n",
    "        df = pd.DataFrame(0, index=range(len(docs)), columns=self.features, dtype=float)\n",
    "        for term in self.features: \n",
    "            for index,doc in enumerate(docs):\n",
    "                df.at[index, term] = self.tf(term=term, doc=doc) * self.idf(term= term, docs=docs)\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def fit_tansform(self, docs: List[str]):\n",
    "        \n",
    "        return self.tf_idf(docs= docs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>analyze</th>\n",
       "      <th>important</th>\n",
       "      <th>courses</th>\n",
       "      <th>of</th>\n",
       "      <th>one</th>\n",
       "      <th>science</th>\n",
       "      <th>scientists</th>\n",
       "      <th>this</th>\n",
       "      <th>best</th>\n",
       "      <th>data</th>\n",
       "      <th>is</th>\n",
       "      <th>fields</th>\n",
       "      <th>most</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.102267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204534</td>\n",
       "      <td>0.102267</td>\n",
       "      <td>0.204534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.102267</td>\n",
       "      <td>0.118275</td>\n",
       "      <td>0.118275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.124993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144559</td>\n",
       "      <td>0.124993</td>\n",
       "      <td>0.124993</td>\n",
       "      <td>0.124993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144559</td>\n",
       "      <td>0.144559</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.124993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        the   analyze  important   courses        of       one   science  \\\n",
       "0  0.102267  0.000000   0.118275  0.000000  0.204534  0.102267  0.204534   \n",
       "1  0.124993  0.000000   0.000000  0.144559  0.124993  0.124993  0.124993   \n",
       "2  0.000000  0.325257   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   scientists      this      best      data        is    fields      most  \n",
       "0    0.000000  0.000000  0.000000  0.090909  0.102267  0.118275  0.118275  \n",
       "1    0.000000  0.144559  0.144559  0.111111  0.124993  0.000000  0.000000  \n",
       "2    0.325257  0.000000  0.000000  0.500000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = CustomTfidfVectorizer()\n",
    "dat = tf_idf.fit_tansform(docs= corpus)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data science is one of the most important fields of science',\n",
       " 'this is one of the best data science courses',\n",
       " 'data scientists analyze data']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data science one important field science',\n",
       " 'one best data science course',\n",
       " 'data scientist analyze data']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = TextNormalizerPipeline()\n",
    "\n",
    "preprocessed_corpus =[normalizer.normalize_text(doc) for doc in corpus]\n",
    "preprocessed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analyze</th>\n",
       "      <th>important</th>\n",
       "      <th>field</th>\n",
       "      <th>one</th>\n",
       "      <th>science</th>\n",
       "      <th>course</th>\n",
       "      <th>best</th>\n",
       "      <th>data</th>\n",
       "      <th>scientist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216838</td>\n",
       "      <td>0.216838</td>\n",
       "      <td>0.187490</td>\n",
       "      <td>0.374980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224988</td>\n",
       "      <td>0.224988</td>\n",
       "      <td>0.260206</td>\n",
       "      <td>0.260206</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.325257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.325257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    analyze  important     field       one   science    course      best  \\\n",
       "0  0.000000   0.216838  0.216838  0.187490  0.374980  0.000000  0.000000   \n",
       "1  0.000000   0.000000  0.000000  0.224988  0.224988  0.260206  0.260206   \n",
       "2  0.325257   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       data  scientist  \n",
       "0  0.166667   0.000000  \n",
       "1  0.200000   0.000000  \n",
       "2  0.500000   0.325257  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf1 = CustomTfidfVectorizer()\n",
    "dat = tf_idf1.fit_tansform(docs= preprocessed_corpus)\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
